{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": "# Unity Catalog-aware serverless tokenization notebook  \n# Uses dbutils.secrets.get() + UC HTTP connections for serverless compatibility\nimport json\nimport os\nfrom pyspark.sql import SparkSession\nfrom pyspark.dbutils import DBUtils\n\n# Initialize Spark session optimized for serverless compute\nspark = SparkSession.builder \\\n    .appName(\"SkyflowTokenization\") \\\n    .config(\"spark.databricks.cluster.profile\", \"serverless\") \\\n    .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n    .getOrCreate()\n    \ndbutils = DBUtils(spark)\n\nprint(f\"✓ Running on Databricks serverless compute\")\nprint(f\"✓ Spark version: {spark.version}\")\n\n# Performance configuration\nMAX_MERGE_BATCH_SIZE = 10000  # Maximum records per MERGE operation\nCOLLECT_BATCH_SIZE = 1000     # Maximum records to collect() from Spark at once\n\n# Define widgets to receive input parameters\ndbutils.widgets.text(\"table_name\", \"\")\ndbutils.widgets.text(\"pii_columns\", \"\")\ndbutils.widgets.text(\"batch_size\", \"\")  # Skyflow API batch size\n\n# Read widget values\ntable_name = dbutils.widgets.get(\"table_name\")\npii_columns = dbutils.widgets.get(\"pii_columns\").split(\",\")\nSKYFLOW_BATCH_SIZE = int(dbutils.widgets.get(\"batch_size\"))\n\nif not table_name or not pii_columns:\n    raise ValueError(\"Both 'table_name' and 'pii_columns' must be provided.\")\n\nprint(f\"Tokenizing table: {table_name}\")\nprint(f\"PII columns: {', '.join(pii_columns)}\")\nprint(f\"Skyflow API batch size: {SKYFLOW_BATCH_SIZE}\")\nprint(f\"MERGE batch size limit: {MAX_MERGE_BATCH_SIZE:,} records\")\nprint(f\"Collect batch size: {COLLECT_BATCH_SIZE:,} records\")\n\n# Extract catalog and schema from table name if fully qualified\nif '.' in table_name:\n    parts = table_name.split('.')\n    if len(parts) == 3:  # catalog.schema.table\n        catalog_name = parts[0]\n        schema_name = parts[1]\n        table_name_only = parts[2]\n        \n        # Set the catalog and schema context for this session\n        print(f\"Setting catalog context to: {catalog_name}\")\n        spark.sql(f\"USE CATALOG {catalog_name}\")\n        spark.sql(f\"USE SCHEMA {schema_name}\")\n        \n        # Use the simple table name for queries since context is set\n        table_name = table_name_only\n        print(f\"✓ Catalog context set, using table name: {table_name}\")\n\nprint(\"✓ Using dbutils.secrets.get() + UC HTTP connections for serverless compatibility\")\n\ndef tokenize_column_values(column_name, values):\n    \"\"\"\n    Tokenize a list of PII values using Unity Catalog HTTP connection.\n    Uses dbutils.secrets.get() and http_request() for serverless compatibility.\n    Returns list of tokens in same order as input values.\n    \"\"\"\n    if not values:\n        return []\n    \n    # Get secrets using dbutils (works in serverless)\n    table_column = dbutils.secrets.get(\"skyflow-secrets\", \"skyflow_table_column\")\n    skyflow_table = dbutils.secrets.get(\"skyflow-secrets\", \"skyflow_table\")\n    \n    # Create records for each value\n    skyflow_records = [{\n        \"fields\": {table_column: str(value)}\n    } for value in values if value is not None]\n\n    # Create Skyflow tokenization payload\n    payload = {\n        \"records\": skyflow_records,\n        \"tokenization\": True\n    }\n\n    print(f\"  Tokenizing {len(skyflow_records)} values for {column_name}\")\n    \n    # Use Unity Catalog HTTP connection via SQL http_request function\n    # Connection base_path is /v1/vaults/{vault_id}, so we add /{table_name}\n    json_payload = json.dumps(payload).replace(\"'\", \"''\")\n    tokenize_path = f\"/{skyflow_table}\"\n    \n    # Execute tokenization via UC connection\n    result_df = spark.sql(f\"\"\"\n        SELECT http_request(\n            conn => 'skyflow_conn',\n            method => 'POST',\n            path => '{tokenize_path}',\n            headers => map(\n                'Content-Type', 'application/json',\n                'Accept', 'application/json'\n            ),\n            json => '{json_payload}'\n        ) as full_response\n    \"\"\")\n    \n    # Parse response\n    full_response = result_df.collect()[0]['full_response']\n    result = json.loads(full_response.text)\n    \n    # Fail fast if API response indicates error\n    if \"error\" in result:\n        raise RuntimeError(f\"Skyflow API error: {result['error']}\")\n    \n    if \"records\" not in result:\n        raise RuntimeError(f\"Invalid Skyflow API response - missing 'records': {result}\")\n    \n    # Extract tokens in order\n    tokens = []\n    for i, record in enumerate(result.get(\"records\", [])):\n        if \"tokens\" in record and table_column in record[\"tokens\"]:\n            token = record[\"tokens\"][table_column]\n            tokens.append(token)\n        else:\n            raise RuntimeError(f\"Tokenization failed for value {i+1} in {column_name}. Record: {record}\")\n    \n    successful_tokens = len([t for i, t in enumerate(tokens) if t and str(t) != str(values[i])])\n    print(f\"    Successfully tokenized {successful_tokens}/{len(values)} values\")\n    \n    return tokens\n\ndef perform_chunked_merge(table_name, column, update_data):\n    \"\"\"\n    Perform MERGE operations in chunks to avoid memory/timeout issues.\n    Returns total number of rows updated.\n    \"\"\"\n    if not update_data:\n        return 0\n    \n    total_updated = 0\n    chunk_size = MAX_MERGE_BATCH_SIZE\n    total_chunks = (len(update_data) + chunk_size - 1) // chunk_size\n    \n    print(f\"  Splitting {len(update_data):,} updates into {total_chunks} MERGE operations (max {chunk_size:,} per chunk)\")\n    \n    for chunk_idx in range(0, len(update_data), chunk_size):\n        chunk_data = update_data[chunk_idx:chunk_idx + chunk_size]\n        chunk_num = (chunk_idx // chunk_size) + 1\n        \n        # Create temporary view for this chunk\n        temp_df = spark.createDataFrame(chunk_data, [\"customer_id\", f\"new_{column}\"])\n        temp_view_name = f\"temp_{column}_chunk_{chunk_num}_{hash(column) % 1000}\"\n        temp_df.createOrReplaceTempView(temp_view_name)\n        \n        # Perform MERGE operation for this chunk\n        merge_sql = f\"\"\"\n            MERGE INTO `{table_name}` AS target\n            USING {temp_view_name} AS source\n            ON target.customer_id = source.customer_id\n            WHEN MATCHED THEN \n                UPDATE SET `{column}` = source.new_{column}\n        \"\"\"\n        \n        spark.sql(merge_sql)\n        chunk_updated = len(chunk_data)\n        total_updated += chunk_updated\n        \n        print(f\"    Chunk {chunk_num}/{total_chunks}: Updated {chunk_updated:,} rows\")\n        \n        # Clean up temp view\n        spark.catalog.dropTempView(temp_view_name)\n    \n    return total_updated\n\n# Process each column individually (streaming approach)\nprint(\"Starting column-by-column tokenization with streaming chunked processing...\")\n\nfor column in pii_columns:\n    print(f\"\\nProcessing column: {column}\")\n    \n    # Get total count first for progress tracking\n    total_count = spark.sql(f\"\"\"\n        SELECT COUNT(*) as count \n        FROM `{table_name}` \n        WHERE `{column}` IS NOT NULL\n    \"\"\").collect()[0]['count']\n    \n    if total_count == 0:\n        print(f\"  No data found in column {column}\")\n        continue\n        \n    print(f\"  Found {total_count:,} total values to tokenize\")\n    \n    # Process in streaming chunks to avoid memory issues\n    all_update_data = []  # Collect all updates before final MERGE\n    processed_count = 0\n    \n    for offset in range(0, total_count, COLLECT_BATCH_SIZE):\n        chunk_size = min(COLLECT_BATCH_SIZE, total_count - offset)\n        print(f\"  Processing chunk {offset//COLLECT_BATCH_SIZE + 1} ({chunk_size:,} records, offset {offset:,})...\")\n        \n        # Get chunk of data from Spark\n        chunk_df = spark.sql(f\"\"\"\n            SELECT customer_id, `{column}` \n            FROM `{table_name}` \n            WHERE `{column}` IS NOT NULL \n            ORDER BY customer_id\n            LIMIT {chunk_size} OFFSET {offset}\n        \"\"\")\n        \n        chunk_rows = chunk_df.collect()\n        if not chunk_rows:\n            continue\n            \n        # Extract customer IDs and values for this chunk\n        chunk_customer_ids = [row['customer_id'] for row in chunk_rows]\n        chunk_column_values = [row[column] for row in chunk_rows]\n        \n        # Tokenize this chunk's values in Skyflow API batches\n        chunk_tokens = []\n        if len(chunk_column_values) <= SKYFLOW_BATCH_SIZE:  # Single API batch\n            chunk_tokens = tokenize_column_values(f\"{column}_chunk_{offset//COLLECT_BATCH_SIZE + 1}\", chunk_column_values)\n        else:  # Multiple API batches within this chunk\n            for i in range(0, len(chunk_column_values), SKYFLOW_BATCH_SIZE):\n                api_batch_values = chunk_column_values[i:i + SKYFLOW_BATCH_SIZE]\n                api_batch_tokens = tokenize_column_values(f\"{column}_chunk_{offset//COLLECT_BATCH_SIZE + 1}_api_{i//SKYFLOW_BATCH_SIZE + 1}\", api_batch_values)\n                chunk_tokens.extend(api_batch_tokens)\n        \n        # Verify token count matches input count (fail fast)\n        if len(chunk_tokens) != len(chunk_customer_ids):\n            raise RuntimeError(f\"Token count mismatch: got {len(chunk_tokens)} tokens for {len(chunk_customer_ids)} input values\")\n        \n        # Collect update data for rows that changed in this chunk\n        chunk_original_map = {chunk_customer_ids[i]: chunk_column_values[i] for i in range(len(chunk_customer_ids))}\n        \n        for i, (customer_id, token) in enumerate(zip(chunk_customer_ids, chunk_tokens)):\n            if token and str(token) != str(chunk_original_map[customer_id]):\n                all_update_data.append((customer_id, token))\n        \n        processed_count += len(chunk_rows)\n        print(f\"    Processed {processed_count:,}/{total_count:,} records ({(processed_count/total_count)*100:.1f}%)\")\n    \n    # Perform final chunked MERGE operations for all collected updates\n    if all_update_data:\n        print(f\"  Performing final chunked MERGE of {len(all_update_data):,} changed rows...\")\n        total_updated = perform_chunked_merge(table_name, column, all_update_data)\n        print(f\"  ✓ Successfully updated {total_updated:,} rows in column {column}\")\n    else:\n        print(f\"  No updates needed - all tokens match original values\")\n\nprint(\"\\nOptimized streaming tokenization completed!\")\n\n# Verify results\nprint(\"\\nFinal verification:\")\nfor column in pii_columns:\n    sample_df = spark.sql(f\"\"\"\n        SELECT `{column}`, COUNT(*) as count \n        FROM `{table_name}` \n        GROUP BY `{column}` \n        LIMIT 3\n    \"\"\")\n    print(f\"\\nSample values in {column}:\")\n    sample_df.show(truncate=False)\n\ntotal_rows = spark.sql(f\"SELECT COUNT(*) as count FROM `{table_name}`\").collect()[0][\"count\"]\nprint(f\"\\nTable size: {total_rows:,} total rows\")\n\nprint(f\"Optimized streaming tokenization completed for {len(pii_columns)} columns\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}