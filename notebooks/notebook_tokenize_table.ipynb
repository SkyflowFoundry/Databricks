{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": "# Unity Catalog-aware serverless tokenization notebook  \n# Optimized version with chunked processing and MERGE operations for maximum performance\nimport requests\nimport os\nfrom pyspark.sql import SparkSession\nfrom pyspark.dbutils import DBUtils\n\n# Initialize Spark session optimized for serverless compute\nspark = SparkSession.builder \\\n    .appName(\"SkyflowTokenization\") \\\n    .config(\"spark.databricks.cluster.profile\", \"serverless\") \\\n    .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n    .getOrCreate()\n    \ndbutils = DBUtils(spark)\n\nprint(f\"✓ Running on Databricks serverless compute\")\nprint(f\"✓ Spark version: {spark.version}\")\n\n# Performance configuration\nMAX_MERGE_BATCH_SIZE = 10000  # Maximum records per MERGE operation\nCOLLECT_BATCH_SIZE = 1000     # Maximum records to collect() from Spark at once\n\n# Define widgets to receive input parameters\ndbutils.widgets.text(\"table_name\", \"\")\ndbutils.widgets.text(\"pii_columns\", \"\")\ndbutils.widgets.text(\"batch_size\", \"25\")  # Skyflow API batch size\n\n# Read widget values\ntable_name = dbutils.widgets.get(\"table_name\")\npii_columns = dbutils.widgets.get(\"pii_columns\").split(\",\")\nSKYFLOW_BATCH_SIZE = int(dbutils.widgets.get(\"batch_size\"))\n\nif not table_name or not pii_columns:\n    raise ValueError(\"Both 'table_name' and 'pii_columns' must be provided.\")\n\nprint(f\"Tokenizing table: {table_name}\")\nprint(f\"PII columns: {', '.join(pii_columns)}\")\nprint(f\"Skyflow API batch size: {SKYFLOW_BATCH_SIZE}\")\nprint(f\"MERGE batch size limit: {MAX_MERGE_BATCH_SIZE:,} records\")\nprint(f\"Collect batch size: {COLLECT_BATCH_SIZE:,} records\")\n\n# Extract catalog and schema from table name if fully qualified\nif '.' in table_name:\n    parts = table_name.split('.')\n    if len(parts) == 3:  # catalog.schema.table\n        catalog_name = parts[0]\n        schema_name = parts[1]\n        table_name_only = parts[2]\n        \n        # Set the catalog and schema context for this session\n        print(f\"Setting catalog context to: {catalog_name}\")\n        spark.sql(f\"USE CATALOG {catalog_name}\")\n        spark.sql(f\"USE SCHEMA {schema_name}\")\n        \n        # Use the simple table name for queries since context is set\n        table_name = table_name_only\n        print(f\"✓ Catalog context set, using table name: {table_name}\")\n\n# Get Skyflow credentials from UC secrets (serverless-compatible)\ntry:\n    SKYFLOW_VAULT_URL = dbutils.secrets.get(scope=\"skyflow-secrets\", key=\"skyflow_vault_url\")\n    SKYFLOW_VAULT_ID = dbutils.secrets.get(scope=\"skyflow-secrets\", key=\"skyflow_vault_id\")\n    SKYFLOW_ACCOUNT_ID = dbutils.secrets.get(scope=\"skyflow-secrets\", key=\"skyflow_account_id\")\n    SKYFLOW_PAT_TOKEN = dbutils.secrets.get(scope=\"skyflow-secrets\", key=\"skyflow_pat_token\")\n    SKYFLOW_TABLE = dbutils.secrets.get(scope=\"skyflow-secrets\", key=\"skyflow_table\")\n    \n    print(\"✓ Successfully retrieved credentials from UC secrets\")\nexcept Exception as e:\n    print(f\"Error retrieving UC secrets: {e}\")\n    raise ValueError(\"Could not retrieve Skyflow credentials from UC secrets\")\n\n# Build API URL\nSKYFLOW_API_URL = f\"{SKYFLOW_VAULT_URL}/v1/vaults/{SKYFLOW_VAULT_ID}/{SKYFLOW_TABLE}\"\n\ndef tokenize_column_values(column_name, values):\n    \"\"\"\n    Tokenize a list of PII values for a specific column via Skyflow API.\n    Simplified - no deduplication, direct 1:1 mapping.\n    Returns list of tokens in same order as input values.\n    \"\"\"\n    if not values:\n        return []\n        \n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\", \n        \"X-SKYFLOW-ACCOUNT-ID\": SKYFLOW_ACCOUNT_ID,\n        \"Authorization\": f\"Bearer {SKYFLOW_PAT_TOKEN}\"\n    }\n\n    # Create records for each value (no deduplication)\n    skyflow_records = [{\n        \"fields\": {\"pii_values\": str(value)}\n    } for value in values if value is not None]\n\n    payload = {\n        \"records\": skyflow_records,\n        \"tokenization\": True\n    }\n\n    try:\n        print(f\"  Tokenizing {len(skyflow_records)} values for {column_name}\")\n        response = requests.post(SKYFLOW_API_URL, headers=headers, json=payload, timeout=30)\n        response.raise_for_status()\n        result = response.json()\n        \n        # Extract tokens in order (1:1 mapping)\n        tokens = []\n        for i, record in enumerate(result.get(\"records\", [])):\n            if \"tokens\" in record and \"pii_values\" in record[\"tokens\"]:\n                token = record[\"tokens\"][\"pii_values\"]\n                tokens.append(token)\n            else:\n                print(f\"    Value {i+1}: failed to tokenize, keeping original\")\n                tokens.append(values[i] if i < len(values) and values[i] is not None else None)\n        \n        return tokens\n        \n    except requests.exceptions.RequestException as e:\n        print(f\"  ❌ ERROR tokenizing {column_name}: {e}\")\n        \n        # Show detailed API error response for troubleshooting\n        if hasattr(e, 'response') and e.response:\n            try:\n                error_details = e.response.json()\n                print(f\"  API Error Details: {error_details}\")\n            except:\n                print(f\"  API Error Response: {e.response.text}\")\n            print(f\"  Status Code: {e.response.status_code}\")\n            print(f\"  Headers: {dict(e.response.headers)}\")\n        \n        # Return original values on error\n        return [str(val) if val is not None else None for val in values]\n    except Exception as e:\n        print(f\"  ❌ UNEXPECTED ERROR tokenizing {column_name}: {e}\")\n        return [str(val) if val is not None else None for val in values]\n\ndef perform_chunked_merge(table_name, column, update_data):\n    \"\"\"\n    Perform MERGE operations in chunks to avoid memory/timeout issues.\n    Returns total number of rows updated.\n    \"\"\"\n    if not update_data:\n        return 0\n    \n    total_updated = 0\n    chunk_size = MAX_MERGE_BATCH_SIZE\n    total_chunks = (len(update_data) + chunk_size - 1) // chunk_size\n    \n    print(f\"  Splitting {len(update_data):,} updates into {total_chunks} MERGE operations (max {chunk_size:,} per chunk)\")\n    \n    for chunk_idx in range(0, len(update_data), chunk_size):\n        chunk_data = update_data[chunk_idx:chunk_idx + chunk_size]\n        chunk_num = (chunk_idx // chunk_size) + 1\n        \n        try:\n            # Create temporary view for this chunk\n            temp_df = spark.createDataFrame(chunk_data, [\"customer_id\", f\"new_{column}\"])\n            temp_view_name = f\"temp_{column}_chunk_{chunk_num}_{hash(column) % 1000}\"\n            temp_df.createOrReplaceTempView(temp_view_name)\n            \n            # Perform MERGE operation for this chunk\n            merge_sql = f\"\"\"\n                MERGE INTO `{table_name}` AS target\n                USING {temp_view_name} AS source\n                ON target.customer_id = source.customer_id\n                WHEN MATCHED THEN \n                    UPDATE SET `{column}` = source.new_{column}\n            \"\"\"\n            \n            spark.sql(merge_sql)\n            chunk_updated = len(chunk_data)\n            total_updated += chunk_updated\n            \n            print(f\"    Chunk {chunk_num}/{total_chunks}: Updated {chunk_updated:,} rows\")\n            \n            # Clean up temp view\n            spark.catalog.dropTempView(temp_view_name)\n            \n        except Exception as e:\n            print(f\"    Error in chunk {chunk_num}: {e}\")\n            print(f\"    Falling back to row-by-row for this chunk...\")\n            \n            # Fallback to row-by-row for this chunk only\n            chunk_fallback_count = 0\n            for customer_id, token in chunk_data:\n                try:\n                    spark.sql(f\"\"\"\n                        UPDATE `{table_name}` \n                        SET `{column}` = '{token}' \n                        WHERE customer_id = '{customer_id}'\n                    \"\"\")\n                    chunk_fallback_count += 1\n                except Exception as row_e:\n                    print(f\"      Error updating customer_id {customer_id}: {row_e}\")\n            \n            total_updated += chunk_fallback_count\n            print(f\"    Chunk {chunk_num} fallback: Updated {chunk_fallback_count} rows\")\n    \n    return total_updated\n\n# Process each column individually (streaming approach)\nprint(\"Starting column-by-column tokenization with streaming chunked processing...\")\n\nfor column in pii_columns:\n    print(f\"\\nProcessing column: {column}\")\n    \n    # Get total count first for progress tracking\n    total_count = spark.sql(f\"\"\"\n        SELECT COUNT(*) as count \n        FROM `{table_name}` \n        WHERE `{column}` IS NOT NULL\n    \"\"\").collect()[0]['count']\n    \n    if total_count == 0:\n        print(f\"  No data found in column {column}\")\n        continue\n        \n    print(f\"  Found {total_count:,} total values to tokenize\")\n    \n    # Process in streaming chunks to avoid memory issues\n    all_update_data = []  # Collect all updates before final MERGE\n    processed_count = 0\n    \n    for offset in range(0, total_count, COLLECT_BATCH_SIZE):\n        chunk_size = min(COLLECT_BATCH_SIZE, total_count - offset)\n        print(f\"  Processing chunk {offset//COLLECT_BATCH_SIZE + 1} ({chunk_size:,} records, offset {offset:,})...\")\n        \n        # Get chunk of data from Spark\n        chunk_df = spark.sql(f\"\"\"\n            SELECT customer_id, `{column}` \n            FROM `{table_name}` \n            WHERE `{column}` IS NOT NULL \n            ORDER BY customer_id\n            LIMIT {chunk_size} OFFSET {offset}\n        \"\"\")\n        \n        chunk_rows = chunk_df.collect()\n        if not chunk_rows:\n            continue\n            \n        # Extract customer IDs and values for this chunk\n        chunk_customer_ids = [row['customer_id'] for row in chunk_rows]\n        chunk_column_values = [row[column] for row in chunk_rows]\n        \n        # Tokenize this chunk's values in Skyflow API batches\n        chunk_tokens = []\n        if len(chunk_column_values) <= SKYFLOW_BATCH_SIZE:  # Single API batch\n            chunk_tokens = tokenize_column_values(f\"{column}_chunk_{offset//COLLECT_BATCH_SIZE + 1}\", chunk_column_values)\n        else:  # Multiple API batches within this chunk\n            for i in range(0, len(chunk_column_values), SKYFLOW_BATCH_SIZE):\n                api_batch_values = chunk_column_values[i:i + SKYFLOW_BATCH_SIZE]\n                api_batch_tokens = tokenize_column_values(f\"{column}_chunk_{offset//COLLECT_BATCH_SIZE + 1}_api_{i//SKYFLOW_BATCH_SIZE + 1}\", api_batch_values)\n                chunk_tokens.extend(api_batch_tokens)\n        \n        if len(chunk_tokens) != len(chunk_customer_ids):\n            print(f\"    Warning: Token count ({len(chunk_tokens):,}) doesn't match chunk row count ({len(chunk_customer_ids):,})\")\n            continue\n        \n        # Collect update data for rows that changed in this chunk\n        chunk_original_map = {chunk_customer_ids[i]: chunk_column_values[i] for i in range(len(chunk_customer_ids))}\n        \n        for i, (customer_id, token) in enumerate(zip(chunk_customer_ids, chunk_tokens)):\n            if token and str(token) != str(chunk_original_map[customer_id]):\n                all_update_data.append((customer_id, token))\n        \n        processed_count += len(chunk_rows)\n        print(f\"    Processed {processed_count:,}/{total_count:,} records ({(processed_count/total_count)*100:.1f}%)\")\n    \n    # Perform final chunked MERGE operations for all collected updates\n    if all_update_data:\n        print(f\"  Performing final chunked MERGE of {len(all_update_data):,} changed rows...\")\n        total_updated = perform_chunked_merge(table_name, column, all_update_data)\n        print(f\"  ✓ Successfully updated {total_updated:,} rows in column {column}\")\n    else:\n        print(f\"  No updates needed - all tokens match original values\")\n\nprint(\"\\nOptimized streaming tokenization completed!\")\n\n# Verify results\nprint(\"\\nFinal verification:\")\nfor column in pii_columns:\n    sample_df = spark.sql(f\"\"\"\n        SELECT `{column}`, COUNT(*) as count \n        FROM `{table_name}` \n        GROUP BY `{column}` \n        LIMIT 3\n    \"\"\")\n    print(f\"\\nSample values in {column}:\")\n    sample_df.show(truncate=False)\n\ntotal_rows = spark.sql(f\"SELECT COUNT(*) as count FROM `{table_name}`\").collect()[0][\"count\"]\nprint(f\"\\nTable size: {total_rows:,} total rows\")\n\ndbutils.notebook.exit(f\"Optimized streaming tokenization completed for {len(pii_columns)} columns\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}