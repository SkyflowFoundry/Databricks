{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import requests\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.dbutils import DBUtils\n",
                "\n",
                "# Initialize Spark session and Databricks utilities\n",
                "spark = SparkSession.builder.appName(\"SkyflowTokenization\").getOrCreate()\n",
                "dbutils = DBUtils(spark)\n",
                "\n",
                "# Define widgets to receive input parameters\n",
                "dbutils.widgets.text(\"table_name\", \"\")\n",
                "dbutils.widgets.text(\"pii_columns\", \"\")\n",
                "\n",
                "# Read widget values\n",
                "table_name = dbutils.widgets.get(\"table_name\")\n",
                "pii_columns = dbutils.widgets.get(\"pii_columns\").split(\",\")\n",
                "\n",
                "if not table_name or not pii_columns:\n",
                "    raise ValueError(\"Both 'table_name' and 'pii_columns' must be provided.\")\n",
                "\n",
                "# Skyflow API details\n",
                "SKYFLOW_API_URL = \"<TODO: SKYFLOW_VAULT_URL>/v1/vaults/<TODO: SKYFLOW_VAULT_ID>/pii\"\n",
                "SKYFLOW_ACCOUNT_ID = \"<TODO: SKYFLOW_ACCOUNT_ID>\"\n",
                "SKYFLOW_BEARER_TOKEN = \"<TODO: SKYFLOW_BEARER_TOKEN>\"\n",
                "\n",
                "def tokenize_batch(values):\n",
                "    \"\"\"\n",
                "    Function to tokenize a batch of PII values via Skyflow API.\n",
                "    All values are already strings from the table schema.\n",
                "    \"\"\"\n",
                "    headers = {\n",
                "        \"Content-Type\": \"application/json\",\n",
                "        \"Accept\": \"application/json\",\n",
                "        \"X-SKYFLOW-ACCOUNT-ID\": SKYFLOW_ACCOUNT_ID,\n",
                "        \"Authorization\": f\"Bearer {SKYFLOW_BEARER_TOKEN}\"\n",
                "    }\n",
                "\n",
                "    # Format records exactly like the successful example\n",
                "    records = [{\n",
                "        \"fields\": {\n",
                "            \"pii\": value\n",
                "        }\n",
                "    } for value in values if value is not None]\n",
                "\n",
                "    payload = {\n",
                "        \"records\": records,\n",
                "        \"tokenization\": True\n",
                "    }\n",
                "\n",
                "    try:\n",
                "        response = requests.post(SKYFLOW_API_URL, headers=headers, json=payload)\n",
                "        response.raise_for_status()\n",
                "        return [record[\"tokens\"][\"pii\"] for record in response.json()[\"records\"]]\n",
                "    except requests.exceptions.RequestException as e:\n",
                "        print(f\"Error tokenizing batch: {e}\")\n",
                "        if hasattr(e.response, 'text'):\n",
                "            print(f\"Response content: {e.response.text}\")\n",
                "        return [\"ERROR\" for _ in values]\n",
                "\n",
                "for column in pii_columns:\n",
                "    # Read distinct non-null PII values\n",
                "    query = f\"SELECT DISTINCT `{column}` FROM `{table_name}` WHERE `{column}` IS NOT NULL\"\n",
                "    df = spark.sql(query)\n",
                "    values = [row[column] for row in df.collect()]\n",
                "\n",
                "    if not values:\n",
                "        print(f\"No PII values found for column: {column}\")\n",
                "        continue\n",
                "\n",
                "    # Tokenize data in batches\n",
                "    batch_size = 25\n",
                "    tokenized_values = []\n",
                "    for i in range(0, len(values), batch_size):\n",
                "        batch = values[i:i + batch_size]\n",
                "        tokenized_values.extend(tokenize_batch(batch))\n",
                "\n",
                "    # Generate and execute update statements\n",
                "    update_statements = [\n",
                "        f\"UPDATE `{table_name}` SET `{column}` = '{token}' WHERE `{column}` = '{value}'\"\n",
                "        for value, token in zip(values, tokenized_values)\n",
                "    ]\n",
                "\n",
                "    for stmt in update_statements:\n",
                "        spark.sql(stmt)\n",
                "\n",
                "    print(f\"Successfully tokenized column: {column}\")\n",
                "\n",
                "dbutils.notebook.exit(f\"Tokenization completed for table `{table_name}` with columns {', '.join(pii_columns)}.\")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
